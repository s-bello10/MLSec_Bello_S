{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Project 1\n",
    "Select five CIFAR-10 (ℓ∞) models from RobustBench and re-evaluate them using AutoAttack under different values of the radius epsilon (e.g., from 1/255 to 16/255, regularly spaced interval including the baseline 8/255), using a subset of 100-200 samples. Compare the resulting robust accuracies and model rankings across these settings. Evaluate the stability of model rankings across different epsilon values. Identify cases where these changes lead to significant rank shifts and discuss what this reveals about the reliability of RobustBench leaderboards.\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torchvision.datasets\n",
    "from robustbench.utils import load_model\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from autoattack import AutoAttack\n",
    "from fractions import Fraction\n",
    "import json\n",
    "from pathlib import Path"
   ],
   "metadata": {
    "id": "Tch2I4N5ON1n",
    "ExecuteTime": {
     "end_time": "2026-01-03T21:47:18.871127Z",
     "start_time": "2026-01-03T21:47:14.395994Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "class EvaluateOnAutoAttack:\n",
    "    \"\"\"\n",
    "    A class to evaluate multiple deep learning models against adversarial attacks\n",
    "    using AutoAttack across different perturbation budgets (epsilon values).\n",
    "    \n",
    "    AutoAttack is an ensemble of four complementary attacks:\n",
    "    - APGD-CE: Auto-PGD with Cross-Entropy loss\n",
    "    - APGD-DLR: Auto-PGD with Difference of Logits Ratio loss\n",
    "    - FAB: Fast Adaptive Boundary attack\n",
    "    - Square Attack: A query-efficient black-box attack\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, models_names, epsilons, dataset, threat_model, device, checkpoint_path, batch_size):\n",
    "        \"\"\"\n",
    "        Initialize the evaluation pipeline.\n",
    "        \n",
    "        Args:\n",
    "            models_names: List of model identifiers from RobustBench\n",
    "            epsilons: List of perturbation budgets as string fractions (e.g., \"8/255\")\n",
    "            dataset: Dataset name (e.g., \"cifar10\")\n",
    "            threat_model: Norm constraint for attacks (\"Linf\", \"L2\", etc.)\n",
    "            device: Computation device (\"cuda\" or \"cpu\")\n",
    "            checkpoint_path: Path to save/load intermediate results\n",
    "            batch_size: Number of samples per batch for evaluation\n",
    "        \"\"\"\n",
    "        # Store configuration parameters as private attributes\n",
    "        self._models_names = models_names\n",
    "        self._epsilons = epsilons\n",
    "        self._dataset = dataset\n",
    "        self._threat_model = threat_model\n",
    "        self._device = device\n",
    "        self._checkpoint_path = checkpoint_path\n",
    "        self._batch_size = batch_size\n",
    "\n",
    "        # Load test data and prepare it for AutoAttack evaluation\n",
    "        self._test_data_loader = self._loadTestDataLoader()\n",
    "        self._x_test, self._y_test = self._prepareTestForAutoAttack()\n",
    "\n",
    "        # Dictionary to cache loaded models and their clean accuracies\n",
    "        self._models_aa_dict = dict()\n",
    "        # Load any previously computed results from checkpoint file\n",
    "        self._results_checkpoint = self._loadResultCheckpoint()\n",
    "\n",
    "    def _loadTestDataLoader(self):\n",
    "        \"\"\"\n",
    "        Load the test dataset and create a DataLoader.\n",
    "        \n",
    "        Returns:\n",
    "            DataLoader: A DataLoader containing a subset of test samples\n",
    "        \"\"\"\n",
    "        if self._dataset == \"cifar10\":\n",
    "            # Load CIFAR-10 test set with tensor transformation\n",
    "            test_dataset = torchvision.datasets.CIFAR10(\n",
    "                transform=torchvision.transforms.ToTensor(),  # Convert PIL images to tensors [0,1]\n",
    "                train=False,  # Use test split, not training\n",
    "                root=\"./data/datasets\",  # Local storage path\n",
    "                download=True,  # Download if not present\n",
    "            )\n",
    "\n",
    "            # Use only first 200 samples for faster evaluation\n",
    "            # This is a common practice for quick robustness benchmarking\n",
    "            test_dataset = Subset(test_dataset, list(range(200)))\n",
    "            return DataLoader(test_dataset, batch_size=self._batch_size, shuffle=False)\n",
    "        else:\n",
    "            # TO-DO: Add support for other datasets (ImageNet, MNIST, etc.)\n",
    "            return None\n",
    "\n",
    "    def _prepareTestForAutoAttack(self):\n",
    "        \"\"\"\n",
    "        Concatenate all batches into single tensors for AutoAttack.\n",
    "        \n",
    "        AutoAttack expects the entire test set as a single tensor rather than\n",
    "        a DataLoader, so we aggregate all batches here.\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (x_test, y_test) tensors on the target device\n",
    "        \"\"\"\n",
    "        all_x = []  # Will store image batches\n",
    "        all_y = []  # Will store label batches\n",
    "\n",
    "        # Iterate through DataLoader and collect all batches\n",
    "        for x, y in self._test_data_loader:\n",
    "            all_x.append(x)\n",
    "            all_y.append(y)\n",
    "\n",
    "        # Concatenate into single tensors and move to device (GPU/CPU)\n",
    "        x_test = torch.cat(all_x).to(self._device)    # shape [N, 3, H, W] for CIFAR-10: [200, 3, 32, 32]\n",
    "        y_test = torch.cat(all_y).to(self._device)     # shape [N] for CIFAR-10: [200]\n",
    "\n",
    "        return x_test, y_test\n",
    "\n",
    "    def _loadResultCheckpoint(self):\n",
    "        \"\"\"\n",
    "        Load previously computed results from checkpoint file.\n",
    "        \n",
    "        This enables resuming interrupted evaluations without re-computing\n",
    "        already completed model/epsilon combinations.\n",
    "        \n",
    "        Returns:\n",
    "            dict: Previously saved results, or empty dict if no checkpoint exists\n",
    "        \"\"\"\n",
    "        if self._checkpoint_path.exists():\n",
    "            with self._checkpoint_path.open(\"r\") as f:\n",
    "                return json.load(f)\n",
    "        return {}\n",
    "\n",
    "    def _saveResultCheckpoint(self):\n",
    "        \"\"\"\n",
    "        Save current results to checkpoint file atomically.\n",
    "        \n",
    "        Uses a temporary file and atomic rename to prevent data corruption\n",
    "        if the process is interrupted during save.\n",
    "        \"\"\"\n",
    "        # Write to temporary file first\n",
    "        tmp = self._checkpoint_path.with_suffix(\".tmp\")\n",
    "        with tmp.open(\"w\") as f:\n",
    "            json.dump(self._results_checkpoint, f, indent=2)\n",
    "        # Atomically replace the original file\n",
    "        tmp.replace(self._checkpoint_path)\n",
    "\n",
    "    def _loadModel(self, model_name):\n",
    "        \"\"\"\n",
    "        Load a model from RobustBench, using cache if already loaded.\n",
    "        \n",
    "        Models are cached to avoid redundant loading when evaluating\n",
    "        the same model across multiple epsilon values.\n",
    "        \n",
    "        Args:\n",
    "            model_name: RobustBench model identifier\n",
    "            \n",
    "        Returns:\n",
    "            nn.Module: The loaded PyTorch model\n",
    "        \"\"\"\n",
    "        if model_name in self._models_aa_dict:\n",
    "            # Return cached model if already loaded\n",
    "            current_model = self._models_aa_dict[model_name][\"model\"]\n",
    "        else:\n",
    "            # Load model from RobustBench repository\n",
    "            current_model = load_model(model_name=model_name, dataset=self._dataset, threat_model=self._threat_model)\n",
    "            current_model.to(self._device)\n",
    "            \n",
    "            # Cache the model and compute its clean accuracy once\n",
    "            self._models_aa_dict[model_name] = dict()\n",
    "            self._models_aa_dict[model_name][\"model\"] = current_model\n",
    "            self._models_aa_dict[model_name][\"clean_acc\"] = self._getCleanAccuracy(current_model, self._test_data_loader)\n",
    "\n",
    "        return current_model\n",
    "\n",
    "    def _getCleanAccuracy(self, current_model, test_data):\n",
    "        \"\"\"\n",
    "        Compute classification accuracy on clean (unperturbed) samples.\n",
    "        \n",
    "        This serves as a baseline to compare against robust accuracy.\n",
    "        A model should have high clean accuracy before being evaluated for robustness.\n",
    "        \n",
    "        Args:\n",
    "            current_model: The PyTorch model to evaluate\n",
    "            test_data: DataLoader containing test samples\n",
    "            \n",
    "        Returns:\n",
    "            float: Accuracy as a value between 0 and 1\n",
    "        \"\"\"\n",
    "        current_model.eval()  # Set to evaluation mode (disables dropout, etc.)\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        # Disable gradient computation for efficiency\n",
    "        with torch.no_grad():\n",
    "            for x, y in test_data:\n",
    "                x = x.to(self._device)\n",
    "                y = y.to(self._device)\n",
    "\n",
    "                # Forward pass to get logits (raw model outputs)\n",
    "                logits = current_model(x)\n",
    "                # Get predicted class (highest logit value)\n",
    "                preds = logits.argmax(dim=1)\n",
    "\n",
    "                # Count correct predictions\n",
    "                correct += (preds == y).sum().item()\n",
    "                total += y.size(0)\n",
    "\n",
    "        return correct / total\n",
    "\n",
    "    def _loadAutoAttack(self, current_model, current_epsilon):\n",
    "        \"\"\"\n",
    "        Initialize AutoAttack adversary with the specified configuration.\n",
    "        \n",
    "        Args:\n",
    "            current_model: The model to attack\n",
    "            current_epsilon: Maximum perturbation budget (L-infinity norm)\n",
    "            \n",
    "        Returns:\n",
    "            AutoAttack: Configured adversary object\n",
    "        \"\"\"\n",
    "        return AutoAttack(\n",
    "            current_model,\n",
    "            norm=self._threat_model,         # Linf = L-infinity norm constraint\n",
    "            eps=current_epsilon,             # Maximum perturbation magnitude\n",
    "            version='standard',              # Uses all 4 attacks: APGD-CE, APGD-DLR, FAB, Square\n",
    "            device=self._device\n",
    "        )\n",
    "\n",
    "    def _startAutoAttack(self, adversary):\n",
    "        \"\"\"\n",
    "        Execute the AutoAttack evaluation.\n",
    "        \n",
    "        AutoAttack will sequentially apply its attacks and return the\n",
    "        adversarial examples that successfully fool the model.\n",
    "        \n",
    "        Args:\n",
    "            adversary: Configured AutoAttack object\n",
    "            \n",
    "        Returns:\n",
    "            Tensor: Adversarial examples (perturbed versions of x_test)\n",
    "        \"\"\"\n",
    "        return adversary.run_standard_evaluation(\n",
    "            self._x_test, self._y_test, bs=self._batch_size\n",
    "        )\n",
    "\n",
    "    def _getRobustAccuracy(self, current_model, x_adv):\n",
    "        \"\"\"\n",
    "        Compute accuracy on adversarial examples (robust accuracy).\n",
    "        \n",
    "        This is the key metric for evaluating adversarial robustness.\n",
    "        It measures what fraction of adversarial examples the model\n",
    "        still classifies correctly.\n",
    "        \n",
    "        Args:\n",
    "            current_model: The model being evaluated\n",
    "            x_adv: Adversarial examples generated by AutoAttack\n",
    "            \n",
    "        Returns:\n",
    "            float: Robust accuracy as a value between 0 and 1\n",
    "        \"\"\"\n",
    "        current_model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = current_model(x_adv)\n",
    "            preds = logits.argmax(1)\n",
    "            # Compare predictions on adversarial inputs to original labels\n",
    "            robust_acc = (preds == self._y_test).float().mean().item()\n",
    "\n",
    "        return robust_acc\n",
    "\n",
    "    def _emptyCache(self, x_adv):\n",
    "        \"\"\"\n",
    "        Free GPU memory after each evaluation.\n",
    "        \n",
    "        AutoAttack can be memory-intensive, so clearing the cache\n",
    "        between runs helps prevent out-of-memory errors.\n",
    "        \n",
    "        Args:\n",
    "            x_adv: Adversarial examples tensor to delete\n",
    "        \"\"\"\n",
    "        del x_adv\n",
    "        import gc\n",
    "        gc.collect()  # Force Python garbage collection\n",
    "        if \"cuda\" in str(self._device):\n",
    "            torch.cuda.empty_cache()  # Clear CUDA memory cache\n",
    "\n",
    "    def _computeRanking(self, epsilon_str):\n",
    "        \"\"\"\n",
    "        Compute and assign rankings to models for a given epsilon.\n",
    "        \n",
    "        Models are ranked by robust accuracy in descending order\n",
    "        (rank 1 = highest robust accuracy = most robust model).\n",
    "        \n",
    "        Args:\n",
    "            epsilon_str: The epsilon value key (e.g., \"8/255\")\n",
    "        \"\"\"\n",
    "        items = self._results_checkpoint[epsilon_str]\n",
    "\n",
    "        # Sort models by robust accuracy (descending order)\n",
    "        sorted_models = sorted(\n",
    "            items.items(),\n",
    "            key=lambda x: x[1][\"robust_acc\"],\n",
    "            reverse=True\n",
    "        )\n",
    "\n",
    "        # Assign ranks: 1 = best (highest robust accuracy)\n",
    "        for rank, (model_name, data) in enumerate(sorted_models, start=1):\n",
    "            data[\"rank\"] = rank\n",
    "\n",
    "    def attackModel(self):\n",
    "        \"\"\"\n",
    "        Main evaluation loop: test all models across all epsilon values.\n",
    "        \n",
    "        This method orchestrates the entire evaluation pipeline:\n",
    "        1. Iterates through each epsilon value\n",
    "        2. For each epsilon, evaluates all models\n",
    "        3. Computes and stores clean accuracy, robust accuracy, and rankings\n",
    "        4. Saves results to checkpoint after each model evaluation\n",
    "        \n",
    "        The checkpoint system allows resuming interrupted evaluations.\n",
    "        \"\"\"\n",
    "        # Outer loop: iterate through perturbation budgets\n",
    "        for epsilon_index, epsilon_str in enumerate(self._epsilons):\n",
    "            # Convert string fraction to float (e.g., \"8/255\" -> 0.0314...)\n",
    "            current_epsilon = float(Fraction(epsilon_str))\n",
    "            # Initialize results dict for this epsilon if not exists\n",
    "            self._results_checkpoint.setdefault(epsilon_str, {})\n",
    "\n",
    "            # Inner loop: evaluate each model at this epsilon\n",
    "            for model_index, model_name in enumerate(self._models_names):\n",
    "                print(f\"----------- CASE ({epsilon_index + 1}.{model_index + 1}) epsilon = {epsilon_str} & model = {model_name} -----------\")\n",
    "                \n",
    "                # Skip if already computed (enables resuming)\n",
    "                if model_name in self._results_checkpoint[epsilon_str]:\n",
    "                    print(\"!! SKIPPED because it's already been computed !!\")\n",
    "                    print(\"\\n\\n\")\n",
    "                    continue\n",
    "\n",
    "                # Load model (from cache or RobustBench)\n",
    "                current_model = self._loadModel(model_name)\n",
    "\n",
    "                # Configure and run AutoAttack\n",
    "                adversary = self._loadAutoAttack(current_model, current_epsilon)\n",
    "                adversary.verbose = True  # Print attack progress\n",
    "                x_adv = self._startAutoAttack(adversary)\n",
    "                \n",
    "                # Compute robust accuracy on adversarial examples\n",
    "                robust_acc = self._getRobustAccuracy(current_model, x_adv)\n",
    "\n",
    "                # Free GPU memory\n",
    "                self._emptyCache(x_adv)\n",
    "\n",
    "                # Get clean accuracy (computed during model loading)\n",
    "                clean_acc = self._models_aa_dict[model_name]['clean_acc']\n",
    "                print(f\"Clean Accuracy: {clean_acc}\\nRobust Accuracy: {robust_acc}\")\n",
    "                print(\"\\n\\n\")\n",
    "\n",
    "                # Store results for this model at this epsilon\n",
    "                self._results_checkpoint[epsilon_str][model_name] = dict()\n",
    "                self._results_checkpoint[epsilon_str][model_name][\"clean_acc\"] = clean_acc\n",
    "                self._results_checkpoint[epsilon_str][model_name][\"robust_acc\"] = robust_acc\n",
    "\n",
    "                # Save checkpoint after each model (enables resume on failure)\n",
    "                self._saveResultCheckpoint()\n",
    "\n",
    "            # After all models evaluated for this epsilon, compute rankings\n",
    "            self._computeRanking(epsilon_str)\n",
    "            self._saveResultCheckpoint()"
   ],
   "metadata": {
    "id": "5L9KgjCsrjsD",
    "ExecuteTime": {
     "end_time": "2026-01-03T21:48:05.520150Z",
     "start_time": "2026-01-03T21:48:05.510246Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "gfi6Z7T5OQA4",
    "ExecuteTime": {
     "end_time": "2026-01-03T21:59:52.026568Z",
     "start_time": "2026-01-03T21:59:52.023059Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# List of RobustBench models to evaluate\n",
    "MODELS = [\n",
    "    \"Carmon2019Unlabeled\",\n",
    "    \"Sehwag2021Proxy_R18\",\n",
    "    \"Rebuffi2021Fixing_R18_ddpm\",\n",
    "    \"Wang2023Better_WRN-28-10\",\n",
    "    \"Cui2023Decoupled_WRN-28-10\"\n",
    "]\n",
    "\n",
    "# Perturbation budgets (epsilon values) to test\n",
    "EPSILONS = [\"1/255\", \"4/255\", \"8/255\", \"12/255\", \"16/255\"]\n",
    "\n",
    "# Batch size for evaluation\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# Path to save/load checkpoint results (enables resuming interrupted runs)\n",
    "CHECKPOINT_PATH = Path(\"./results_checkpoint.json\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Create the evaluation pipeline with specified configuration\n",
    "# This will:\n",
    "# 1. Download CIFAR-10 test set (if not already present)\n",
    "# 2. Create a subset of 200 samples for evaluation\n",
    "# 3. Load any existing checkpoint results\n",
    "auto_attack = EvaluateOnAutoAttack(\n",
    "    models_names = MODELS,           # List of 5 RobustBench models to evaluate\n",
    "    epsilons = EPSILONS,             # List of 5 epsilon values to test\n",
    "    dataset=\"cifar10\",               # CIFAR-10 dataset (32x32 RGB images, 10 classes)\n",
    "    threat_model=\"Linf\",             # L-infinity norm (max pixel change bounded by epsilon)\n",
    "    device=DEVICE,                   # GPU or CPU\n",
    "    checkpoint_path=CHECKPOINT_PATH, # File to save/resume results\n",
    "    batch_size=BATCH_SIZE            # Batch size for forward passes\n",
    ")"
   ],
   "metadata": {
    "id": "r8fYGvbqy9yZ",
    "ExecuteTime": {
     "end_time": "2026-01-03T21:59:56.163353Z",
     "start_time": "2026-01-03T21:59:55.641123Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Execute AutoAttack on all models across all epsilon values\n",
    "# This will evaluate 5 models x 5 epsilons = 25 total evaluations\n",
    "# Each evaluation runs 4 attacks (APGD-CE, APGD-DLR, FAB, Square Attack)\n",
    "# \n",
    "# Progress is checkpointed after each model, so if interrupted:\n",
    "# - Re-run this cell to resume from where it stopped\n",
    "# - Already-computed results will be skipped automatically\n",
    "#\n",
    "# Output includes:\n",
    "# - Clean accuracy: Performance on unperturbed test samples\n",
    "# - Robust accuracy: Performance on adversarial examples\n",
    "# - Rankings: Models ranked by robust accuracy at each epsilon\n",
    "auto_attack.attackModel()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KEOOhif_6PFu",
    "outputId": "eeb309d1-b46f-4bed-edaa-279937001ef7",
    "ExecuteTime": {
     "end_time": "2026-01-03T23:54:41.878038Z",
     "start_time": "2026-01-03T21:59:57.402680Z"
    }
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
